{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# begin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "epochs = 20\n",
    "def one_hot_encode(y):\n",
    "    l = list()\n",
    "    for item in y:\n",
    "        c = [0. for i in range(10)]\n",
    "        c[item] = 1.\n",
    "        l.append(c)\n",
    "    return np.array(l)\n",
    "def make_model(input_shape):\n",
    "    input_tensor = Input(input_shape)\n",
    "    x = input_tensor\n",
    "    x = Dropout(0.5)(x)\n",
    "    x = Dense(512, activation='relu')(x)  # 增加一个隐藏层\n",
    "    x = Dense(10, activation='softmax')(x)\n",
    "    model = Model(input_tensor, x)\n",
    "    return model\n",
    "X_train = []\n",
    "X_valid = []\n",
    "\n",
    "premodels = [\n",
    "    \"bottleneck_ResNet50.h5\",\n",
    "    \"bottleneck_Xception.h5\",\n",
    "    \"bottleneck_InceptionV3.h5\",\n",
    "]\n",
    "\n",
    "for filename in premodels:\n",
    "    print('------------------'+filename)\n",
    "    with h5py.File(os.path.join(\"models\", tag, filename), 'r') as h:\n",
    "        X_train.append(np.array(h['train']))\n",
    "        X_valid.append(np.array(h['valid']))\n",
    "        y_train = np.array(h['label'])\n",
    "        y_valid = np.array(h['valid_label'])\n",
    "\n",
    "X_train = np.concatenate(X_train, axis=1)\n",
    "X_valid = np.concatenate(X_valid, axis=1)\n",
    "\n",
    "X_train, y_train = shuffle(X_train, y_train)\n",
    "y_train = one_hot_encode(y_train)\n",
    "X_valid, y_valid = shuffle(X_valid, y_valid)\n",
    "y_valid = one_hot_encode(y_valid)\n",
    "model_mix = make_model(X_train.shape[1:])\n",
    "\n",
    "tensorboard_callback = keras.callbacks.TensorBoard()\n",
    "\n",
    "print(\"Adam\")\n",
    "model_mix.compile(optimizer=Adam(lr=0.00001), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "model_mix.fit(X_train, y_train, batch_size=128, epochs=10, validation_data=(X_valid,y_valid), callbacks=[tensorboard_callback])\n",
    "model_mix.save(\"models/mixed-model.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------bottleneck_ResNet50.h5\n",
      "------------------bottleneck_Xception.h5\n",
      "------------------bottleneck_InceptionV3.h5\n",
      "Epoch 1/20\n",
      "163/163 [==============================] - 5s 29ms/step - loss: 0.0682 - accuracy: 0.9846 - val_loss: 0.7252 - val_accuracy: 0.7575\n",
      "Epoch 2/20\n",
      "163/163 [==============================] - 5s 28ms/step - loss: 0.0026 - accuracy: 0.9994 - val_loss: 0.8314 - val_accuracy: 0.7495\n",
      "Epoch 3/20\n",
      "163/163 [==============================] - 5s 28ms/step - loss: 0.0019 - accuracy: 0.9995 - val_loss: 0.7875 - val_accuracy: 0.7618\n",
      "Epoch 4/20\n",
      "163/163 [==============================] - 5s 28ms/step - loss: 0.0018 - accuracy: 0.9994 - val_loss: 0.8098 - val_accuracy: 0.7740\n",
      "Epoch 5/20\n",
      "163/163 [==============================] - 5s 28ms/step - loss: 0.0011 - accuracy: 0.9998 - val_loss: 0.8514 - val_accuracy: 0.7599\n",
      "Epoch 6/20\n",
      "163/163 [==============================] - 5s 28ms/step - loss: 7.3260e-04 - accuracy: 0.9999 - val_loss: 1.2439 - val_accuracy: 0.7141\n",
      "Epoch 7/20\n",
      "163/163 [==============================] - 5s 29ms/step - loss: 6.6706e-04 - accuracy: 0.9999 - val_loss: 1.0384 - val_accuracy: 0.7630\n",
      "Epoch 8/20\n",
      "163/163 [==============================] - 5s 29ms/step - loss: 5.7450e-04 - accuracy: 0.9999 - val_loss: 1.2204 - val_accuracy: 0.7282\n",
      "Epoch 9/20\n",
      "163/163 [==============================] - 5s 28ms/step - loss: 4.5609e-04 - accuracy: 0.9999 - val_loss: 0.9751 - val_accuracy: 0.7630\n",
      "Epoch 10/20\n",
      "163/163 [==============================] - 5s 28ms/step - loss: 6.1741e-04 - accuracy: 0.9998 - val_loss: 1.2084 - val_accuracy: 0.7398\n",
      "Epoch 11/20\n",
      "163/163 [==============================] - 5s 28ms/step - loss: 4.6087e-04 - accuracy: 0.9999 - val_loss: 1.1390 - val_accuracy: 0.7489\n",
      "Epoch 12/20\n",
      "163/163 [==============================] - 5s 28ms/step - loss: 3.3989e-04 - accuracy: 0.9999 - val_loss: 0.8983 - val_accuracy: 0.7935\n",
      "Epoch 13/20\n",
      "163/163 [==============================] - 5s 29ms/step - loss: 3.6175e-04 - accuracy: 0.9999 - val_loss: 0.9409 - val_accuracy: 0.8045\n",
      "Epoch 14/20\n",
      "163/163 [==============================] - 5s 29ms/step - loss: 1.6967e-04 - accuracy: 1.0000 - val_loss: 1.1346 - val_accuracy: 0.7593\n",
      "Epoch 15/20\n",
      "163/163 [==============================] - 5s 29ms/step - loss: 2.7513e-04 - accuracy: 1.0000 - val_loss: 1.2384 - val_accuracy: 0.7624\n",
      "Epoch 16/20\n",
      "163/163 [==============================] - 5s 28ms/step - loss: 2.3492e-04 - accuracy: 1.0000 - val_loss: 1.2303 - val_accuracy: 0.7581\n",
      "Epoch 17/20\n",
      "163/163 [==============================] - 5s 28ms/step - loss: 2.5296e-04 - accuracy: 1.0000 - val_loss: 1.2222 - val_accuracy: 0.7459\n",
      "Epoch 18/20\n",
      "163/163 [==============================] - 5s 28ms/step - loss: 1.2149e-04 - accuracy: 1.0000 - val_loss: 1.1603 - val_accuracy: 0.7624\n",
      "Epoch 19/20\n",
      "163/163 [==============================] - 5s 29ms/step - loss: 6.9335e-05 - accuracy: 1.0000 - val_loss: 1.3005 - val_accuracy: 0.7434\n",
      "Epoch 20/20\n",
      "163/163 [==============================] - 5s 28ms/step - loss: 8.7663e-05 - accuracy: 1.0000 - val_loss: 1.1397 - val_accuracy: 0.7642\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import h5py\n",
    "from sklearn.utils import shuffle\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import Model, Input\n",
    "from tensorflow.keras.layers import Dense, Dropout, Flatten\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import TensorBoard, ModelCheckpoint\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "# 设置超参数\n",
    "batch_size = 128\n",
    "epochs = 20\n",
    "\n",
    "# 数据处理函数：使用Keras的to_categorical来代替one_hot_encode\n",
    "def one_hot_encode(y):\n",
    "    return to_categorical(y, num_classes=10)\n",
    "\n",
    "# 创建混合模型\n",
    "def create_model(input_shape):\n",
    "    # 输入层\n",
    "    input_tensor = Input(input_shape)\n",
    "\n",
    "    # 增加Dropout层和隐藏层\n",
    "    x = Dropout(0.5)(input_tensor)  # 对输入数据进行Dropout\n",
    "    x = Dense(512, activation='relu')(x)\n",
    "    \n",
    "    # 输出层\n",
    "    output = Dense(10, activation='softmax')(x)\n",
    "    \n",
    "    # 创建最终模型\n",
    "    model = Model(inputs=input_tensor, outputs=output)\n",
    "    return model\n",
    "\n",
    "# 读取数据\n",
    "X_train = []\n",
    "X_valid = []\n",
    "y_train = None\n",
    "y_valid = None\n",
    "\n",
    "premodels = [\n",
    "    \"bottleneck_ResNet50.h5\",\n",
    "    \"bottleneck_Xception.h5\",\n",
    "    \"bottleneck_InceptionV3.h5\",\n",
    "]\n",
    "\n",
    "for filename in premodels:\n",
    "    print('------------------' + filename)\n",
    "    with h5py.File(os.path.join(\"models\", tag, filename), 'r') as h:\n",
    "        X_train.append(np.array(h['train']))\n",
    "        X_valid.append(np.array(h['valid']))\n",
    "        y_train = np.array(h['label'])\n",
    "        y_valid = np.array(h['valid_label'])\n",
    "\n",
    "# 拼接多个模型的特征\n",
    "X_train = np.concatenate(X_train, axis=1)\n",
    "X_valid = np.concatenate(X_valid, axis=1)\n",
    "\n",
    "# Shuffle 数据\n",
    "X_train, y_train = shuffle(X_train, y_train)\n",
    "y_train = one_hot_encode(y_train)\n",
    "X_valid, y_valid = shuffle(X_valid, y_valid)\n",
    "y_valid = one_hot_encode(y_valid)\n",
    "\n",
    "# 创建混合模型\n",
    "model_mix = create_model(X_train.shape[1:])  # 直接使用已拼接的特征\n",
    "\n",
    "# 使用TensorBoard回调\n",
    "tensorboard_callback = TensorBoard(log_dir='./logs')\n",
    "\n",
    "# 使用ModelCheckpoint回调来保存最佳模型\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "    'models/mixed-model-best.h5',\n",
    "    save_best_only=True,\n",
    "    monitor='val_loss',\n",
    "    mode='min'\n",
    ")\n",
    "\n",
    "# 编译模型\n",
    "model_mix.compile(optimizer=Adam(learning_rate=0.0001), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# 训练模型\n",
    "model_mix.fit(\n",
    "    X_train, y_train,\n",
    "    batch_size=batch_size,\n",
    "    epochs=epochs,\n",
    "    validation_data=(X_valid, y_valid),\n",
    "    callbacks=[tensorboard_callback, checkpoint_callback]\n",
    ")\n",
    "\n",
    "# 保存最终模型\n",
    "model_mix.save(\"models/mixed-model.h5\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.12.0\n",
      "2.12.0\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "print(keras.__version__)\n",
    "import tensorflow as tf\n",
    "print(tf.__version__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 00000000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import h5py\n",
    "import numpy as np\n",
    "from sklearn.utils import shuffle\n",
    "import pandas as pd\n",
    "\n",
    "import keras\n",
    "from keras.preprocessing.image import *\n",
    "from keras.optimizers import *\n",
    "from keras.callbacks import *\n",
    "\n",
    "np.random.seed(2017)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the ipynb is refer: \n",
    "Create the premodel by ResNet50,Xception, InceptionV3, VGG16, VGG19"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "# dir = \"/ext/Data/distracted_driver_detection/\"\n",
    "# dir = \"F:\\BBBBBBBBBBBBBBBBB\\Datas\"\n",
    "dir = \"F:\\JupyterWorkSpace\\BBBBBBS\"\n",
    "tag = \"finetune\"\n",
    "\n",
    "def one_hot_encode(y):\n",
    "    l = list()\n",
    "    for item in y:\n",
    "        c = [0. for i in range(10)]\n",
    "        c[item] = 1.\n",
    "        l.append(c)\n",
    "    return np.array(l)\n",
    "    \n",
    "from keras.models import *\n",
    "from keras.layers import *\n",
    "\n",
    "# def make_model(input_shape):\n",
    "\n",
    "#     input_tensor = Input(input_shape)\n",
    "#     x = input_tensor\n",
    "#     x = Dropout(0.5)(x)\n",
    "#     x = Dense(10, activation='softmax')(x)\n",
    "#     model = Model(input_tensor, x)\n",
    "#     return model\n",
    "\n",
    "def make_model(input_shape):\n",
    "    input_tensor = Input(input_shape)\n",
    "    x = input_tensor\n",
    "    x = Dropout(0.5)(x)\n",
    "    x = Dense(512, activation='relu')(x)  # 增加一个隐藏层\n",
    "    x = Dense(10, activation='softmax')(x)\n",
    "    model = Model(input_tensor, x)\n",
    "    return model\n",
    "\n",
    "\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "epochs = 20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Combine multi-modles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------bottleneck_ResNet50.h5\n",
      "------------------bottleneck_Xception.h5\n",
      "------------------bottleneck_InceptionV3.h5\n",
      "(20787, 6144)\n",
      "(20787, 10)\n",
      "(1637, 6144)\n",
      "(1637, 10)\n"
     ]
    }
   ],
   "source": [
    "import h5py\n",
    "import numpy as np\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import train_test_split\n",
    "np.random.seed(2017)\n",
    "\n",
    "X_train = []\n",
    "X_valid = []\n",
    "\n",
    "premodels = [\n",
    "    \"bottleneck_ResNet50.h5\",\n",
    "    \"bottleneck_Xception.h5\",\n",
    "    \"bottleneck_InceptionV3.h5\",\n",
    "]\n",
    "\n",
    "for filename in premodels:\n",
    "    print('------------------'+filename)\n",
    "    with h5py.File(os.path.join(\"models\", tag, filename), 'r') as h:\n",
    "        X_train.append(np.array(h['train']))\n",
    "        X_valid.append(np.array(h['valid']))\n",
    "        y_train = np.array(h['label'])\n",
    "        y_valid = np.array(h['valid_label'])\n",
    "\n",
    "X_train = np.concatenate(X_train, axis=1)\n",
    "X_valid = np.concatenate(X_valid, axis=1)\n",
    "\n",
    "X_train, y_train = shuffle(X_train, y_train)\n",
    "y_train = one_hot_encode(y_train)\n",
    "X_valid, y_valid = shuffle(X_valid, y_valid)\n",
    "y_valid = one_hot_encode(y_valid)\n",
    "\n",
    "print(X_train.shape)\n",
    "print(y_train.shape)\n",
    "print(X_valid.shape)\n",
    "print(y_valid.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6144,)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adam\n",
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\13904\\anaconda3\\lib\\site-packages\\keras\\optimizers\\legacy\\adam.py:117: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "163/163 [==============================] - 3s 16ms/step - loss: 0.5746 - accuracy: 0.8794 - val_loss: 0.7015 - val_accuracy: 0.7514\n",
      "Epoch 2/10\n",
      "163/163 [==============================] - 2s 15ms/step - loss: 0.0401 - accuracy: 0.9984 - val_loss: 0.6447 - val_accuracy: 0.7618\n",
      "Epoch 3/10\n",
      "163/163 [==============================] - 2s 15ms/step - loss: 0.0182 - accuracy: 0.9987 - val_loss: 0.6543 - val_accuracy: 0.7587\n",
      "Epoch 4/10\n",
      "163/163 [==============================] - 2s 15ms/step - loss: 0.0117 - accuracy: 0.9989 - val_loss: 0.6658 - val_accuracy: 0.7587\n",
      "Epoch 5/10\n",
      "163/163 [==============================] - 2s 15ms/step - loss: 0.0085 - accuracy: 0.9990 - val_loss: 0.6837 - val_accuracy: 0.7575\n",
      "Epoch 6/10\n",
      "163/163 [==============================] - 2s 15ms/step - loss: 0.0069 - accuracy: 0.9990 - val_loss: 0.6953 - val_accuracy: 0.7569\n",
      "Epoch 7/10\n",
      "163/163 [==============================] - 2s 15ms/step - loss: 0.0054 - accuracy: 0.9993 - val_loss: 0.7180 - val_accuracy: 0.7544\n",
      "Epoch 8/10\n",
      "163/163 [==============================] - 2s 15ms/step - loss: 0.0049 - accuracy: 0.9990 - val_loss: 0.6952 - val_accuracy: 0.7605\n",
      "Epoch 9/10\n",
      "163/163 [==============================] - 2s 15ms/step - loss: 0.0041 - accuracy: 0.9992 - val_loss: 0.7269 - val_accuracy: 0.7544\n",
      "Epoch 10/10\n",
      "163/163 [==============================] - 2s 15ms/step - loss: 0.0039 - accuracy: 0.9993 - val_loss: 0.7410 - val_accuracy: 0.7532\n",
      "model save successed\n"
     ]
    }
   ],
   "source": [
    "model_mix = make_model(X_train.shape[1:])\n",
    "\n",
    "tensorboard_callback = keras.callbacks.TensorBoard()\n",
    "\n",
    "print(\"Adam\")\n",
    "model_mix.compile(optimizer=Adam(lr=0.00001), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "model_mix.fit(X_train, y_train, batch_size=128, epochs=10, validation_data=(X_valid,y_valid), callbacks=[tensorboard_callback])\n",
    "# print(\"RMSprop\")\n",
    "# model_mix.compile(optimizer=RMSprop(lr=1*0.00001), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "# model_mix.fit(X_train, y_train, batch_size=batch_size, epochs=20, validation_data=(X_valid,y_valid))\n",
    "\n",
    "model_mix.save(\"models/mixed-model.h5\")\n",
    "print(\"model save successed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.engine.functional.Functional at 0x1cd40c057e0>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_mix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "# def gen_kaggle_csv(model, X_test,  model_image_size, csv_name):\n",
    "#     y_pred = model.predict(X_test, verbose=1)\n",
    "#     print(y_pred[:3])\n",
    "#     y_pred = y_pred.clip(min=0.005, max=0.995)\n",
    "#     print()\n",
    "#     print(y_pred[:3])\n",
    "\n",
    "#     gen = ImageDataGenerator()\n",
    "#     test_generator = gen.flow_from_directory(dir  + \"/test/\", (model_image_size, model_image_size), shuffle=False, \n",
    "#                                              batch_size=16, class_mode=None)\n",
    "\n",
    "#     l = list()\n",
    "#     for i, fname in enumerate(test_generator.filenames):\n",
    "#         name = fname[fname.rfind('/')+1:]\n",
    "#         l.append( [name, *y_pred[i]] )\n",
    "\n",
    "#     l = np.array(l)\n",
    "#     data = {'img': l[:,0]}\n",
    "#     for i in range(10):\n",
    "#         data[\"c%d\"%i] = l[:,i+1]\n",
    "#     df = pd.DataFrame(data, columns=['img'] + ['c%d'%i for i in range(10)])\n",
    "#     df.head(10)\n",
    "#     df = df.sort_values(by='img')\n",
    "#     df.to_csv(csv_name, index=None, float_format='%.3f')\n",
    "    \n",
    "    \n",
    "#     print(f\"Number of files: {len(test_generator.filenames)}\")\n",
    "#     print(f\"Number of predictions: {len(y_pred)}\")\n",
    "\n",
    "# print(\"done\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "import os\n",
    "\n",
    "def gen_kaggle_csv(model, X_test, model_image_size, csv_name):\n",
    "    # 对测试数据进行预测\n",
    "    y_pred = model.predict(X_test, verbose=1)\n",
    "    print(y_pred[:3])  # 打印前3个预测结果\n",
    "    \n",
    "    # 对预测结果进行裁剪，确保它们在合理范围内\n",
    "    y_pred = y_pred.clip(min=0.005, max=0.995)\n",
    "    print()\n",
    "    print(y_pred[:3])  # 打印裁剪后的前3个预测结果\n",
    "\n",
    "    # 创建图像数据生成器\n",
    "    gen = ImageDataGenerator()\n",
    "\n",
    "    # 使用 flow_from_directory 加载测试数据集，不打乱顺序，设置目标大小\n",
    "    test_generator = gen.flow_from_directory(\n",
    "        dir + \"/test/\",  # 测试集文件夹路径\n",
    "        (model_image_size, model_image_size),  # 调整图像大小\n",
    "        shuffle=False,  # 不打乱图像顺序\n",
    "        batch_size=16,  # 设置每批处理的图像数量\n",
    "        class_mode=None  # 因为没有标签，设置为 None\n",
    "    )\n",
    "\n",
    "    # 创建一个空列表来存储文件名和预测值\n",
    "    l = list()\n",
    "\n",
    "    # 遍历测试集的文件名，提取图片名称并与预测结果结合\n",
    "    for i, fname in enumerate(test_generator.filenames):\n",
    "        name = os.path.basename(fname)  # 获取文件名，不包含路径部分\n",
    "        l.append([name, *y_pred[i]])\n",
    "\n",
    "    # 将列表转换为 NumPy 数组\n",
    "    l = np.array(l)\n",
    "\n",
    "    # 创建字典，存储图片名称和预测概率\n",
    "    data = {'img': l[:, 0]}  # 图片名称\n",
    "    for i in range(10):\n",
    "        data[\"c%d\" % i] = l[:, i + 1]  # 每个类别的预测概率\n",
    "\n",
    "    # 将字典转换为 DataFrame，列名是图片名称和类别预测概率\n",
    "    df = pd.DataFrame(data, columns=['img'] + ['c%d' % i for i in range(10)])\n",
    "\n",
    "    # 打印前10行数据检查\n",
    "    print(df.head(10))\n",
    "\n",
    "    # 按照图片名称排序\n",
    "    df = df.sort_values(by='img')\n",
    "\n",
    "    # 将 DataFrame 保存为 CSV 文件\n",
    "    df.to_csv(csv_name, index=None, float_format='%.3f')\n",
    "\n",
    "    # 打印测试集中图片数量和预测结果数量，进行简单的检查\n",
    "    print(f\"Number of files: {len(test_generator.filenames)}\")\n",
    "    print(f\"Number of predictions: {len(y_pred)}\")\n",
    "\n",
    "# 打印完成信息\n",
    "print(\"done\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------bottleneck_ResNet50_test.h5\n",
      "------------------bottleneck_Xception_test.h5\n",
      "------------------bottleneck_InceptionV3_test.h5\n",
      "2492/2492 [==============================] - 6s 2ms/step\n",
      "[[0.01632781 0.01399498 0.02791261 0.02079366 0.04007988 0.73943186\n",
      "  0.06469563 0.02096968 0.02348928 0.03230467]\n",
      " [0.04596654 0.01829088 0.01733022 0.01399086 0.05013722 0.68175805\n",
      "  0.06910319 0.01904308 0.07859012 0.00578985]\n",
      " [0.16679934 0.12803547 0.02558471 0.04717511 0.08069336 0.03078406\n",
      "  0.11685694 0.07124319 0.07661813 0.25620964]]\n",
      "\n",
      "[[0.01632781 0.01399498 0.02791261 0.02079366 0.04007988 0.73943186\n",
      "  0.06469563 0.02096968 0.02348928 0.03230467]\n",
      " [0.04596654 0.01829088 0.01733022 0.01399086 0.05013722 0.68175805\n",
      "  0.06910319 0.01904308 0.07859012 0.00578985]\n",
      " [0.16679934 0.12803547 0.02558471 0.04717511 0.08069336 0.03078406\n",
      "  0.11685694 0.07124319 0.07661813 0.25620964]]\n",
      "Found 79726 images belonging to 1 classes.\n",
      "              img           c0           c1            c2            c3  \\\n",
      "0       img_1.jpg  0.016327806  0.013994975   0.027912607   0.020793656   \n",
      "1      img_10.jpg   0.04596654  0.018290883   0.017330222   0.013990861   \n",
      "2     img_100.jpg   0.16679934   0.12803547   0.025584705    0.04717511   \n",
      "3    img_1000.jpg  0.012849577  0.009418244   0.045903195   0.005041465   \n",
      "4  img_100000.jpg  0.027667815  0.027664777  0.0066148676     0.8133059   \n",
      "5  img_100001.jpg        0.005  0.009799722  0.0050784685     0.9083956   \n",
      "6  img_100002.jpg  0.009747823   0.12417581   0.032379318     0.1155507   \n",
      "7  img_100003.jpg  0.105603315   0.10637675    0.02009264   0.038318895   \n",
      "8  img_100004.jpg  0.005332319  0.009292066   0.017872045         0.005   \n",
      "9  img_100005.jpg        0.005  0.019659905         0.005  0.0057013384   \n",
      "\n",
      "            c4           c5           c6            c7           c8  \\\n",
      "0   0.04007988   0.73943186  0.064695634   0.020969681  0.023489282   \n",
      "1  0.050137218   0.68175805   0.06910319   0.019043077  0.078590125   \n",
      "2   0.08069336   0.03078406   0.11685694    0.07124319  0.076618135   \n",
      "3   0.03541654  0.009265068  0.022952924   0.005376288   0.83573496   \n",
      "4   0.05917728  0.012587685  0.026511395  0.0077196034  0.013063619   \n",
      "5  0.016916769  0.006194542  0.020919515         0.005   0.01474655   \n",
      "6   0.07640665   0.02934696  0.063580416   0.011527772   0.49976867   \n",
      "7  0.035071287   0.15713036  0.082137115   0.060897656   0.09003544   \n",
      "8  0.025249872  0.031942125   0.87593883         0.005  0.025337381   \n",
      "9        0.005  0.008456317  0.008070382    0.93862176        0.005   \n",
      "\n",
      "             c9  \n",
      "0    0.03230467  \n",
      "1  0.0057898522  \n",
      "2    0.25620964  \n",
      "3   0.018041754  \n",
      "4   0.005687019  \n",
      "5   0.009496595  \n",
      "6   0.037515946  \n",
      "7    0.30433655  \n",
      "8         0.005  \n",
      "9         0.005  \n",
      "Number of files: 79726\n",
      "Number of predictions: 79726\n"
     ]
    }
   ],
   "source": [
    "test_premodels = [\n",
    "     \"bottleneck_ResNet50_test.h5\", \n",
    "     \"bottleneck_Xception_test.h5\", \n",
    "     \"bottleneck_InceptionV3_test.h5\",\n",
    "]\n",
    "X_test = []\n",
    "for filename in test_premodels:\n",
    "    print('------------------'+filename)\n",
    "    with h5py.File(os.path.join(\"models\", tag, filename), 'r') as h:\n",
    "        X_test.append(np.array(h['test']))\n",
    "        \n",
    "X_test = np.concatenate(X_test, axis=1)\n",
    "gen_kaggle_csv(model_mix, X_test,  320, 'csv/mixed-pred-first.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 只关于输出结果部分的汇总（假定已经得到了bottleneck_ResNet50_test等）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n",
      "------------------bottleneck_ResNet50_test.h5\n",
      "------------------bottleneck_Xception_test.h5\n",
      "------------------bottleneck_InceptionV3_test.h5\n",
      "2492/2492 [==============================] - 6s 2ms/step\n",
      "[[0.0063405  0.01604431 0.02785161 0.02301869 0.03530462 0.8023355\n",
      "  0.02995177 0.01099865 0.01762458 0.03052977]\n",
      " [0.02174795 0.01262218 0.02817013 0.01803399 0.02463043 0.81699604\n",
      "  0.01878481 0.00956798 0.04342525 0.00602125]\n",
      " [0.16492495 0.07910203 0.05841956 0.06612758 0.09217478 0.03954029\n",
      "  0.07231129 0.11545973 0.11070835 0.20123146]]\n",
      "\n",
      "[[0.0063405  0.01604431 0.02785161 0.02301869 0.03530462 0.8023355\n",
      "  0.02995177 0.01099865 0.01762458 0.03052977]\n",
      " [0.02174795 0.01262218 0.02817013 0.01803399 0.02463043 0.81699604\n",
      "  0.01878481 0.00956798 0.04342525 0.00602125]\n",
      " [0.16492495 0.07910203 0.05841956 0.06612758 0.09217478 0.03954029\n",
      "  0.07231129 0.11545973 0.11070835 0.20123146]]\n",
      "Found 79726 images belonging to 1 classes.\n",
      "              img           c0           c1           c2           c3  \\\n",
      "0       img_1.jpg  0.006340498  0.016044306  0.027851613  0.023018694   \n",
      "1      img_10.jpg  0.021747952  0.012622183  0.028170126  0.018033987   \n",
      "2     img_100.jpg   0.16492495   0.07910203  0.058419555   0.06612758   \n",
      "3    img_1000.jpg  0.007711893  0.021611858   0.04145989   0.01472676   \n",
      "4  img_100000.jpg  0.038632102  0.035372384  0.021344135   0.75854206   \n",
      "5  img_100001.jpg        0.005  0.024344582  0.008840434   0.85518587   \n",
      "6  img_100002.jpg        0.005    0.2963664  0.016996251   0.14364792   \n",
      "7  img_100003.jpg   0.11359701   0.11046195   0.04238068  0.052400645   \n",
      "8  img_100004.jpg        0.005  0.013597043  0.033391807  0.016658684   \n",
      "9  img_100005.jpg        0.005  0.008914292        0.005        0.005   \n",
      "\n",
      "            c4           c5           c6            c7           c8  \\\n",
      "0   0.03530462    0.8023355  0.029951766   0.010998654  0.017624576   \n",
      "1  0.024630425   0.81699604  0.018784812    0.00956798   0.04342525   \n",
      "2  0.092174776  0.039540287   0.07231129   0.115459725   0.11070835   \n",
      "3  0.026342915  0.016970841  0.021685371   0.021484131   0.81357014   \n",
      "4  0.054481525    0.0244807  0.018291352    0.02391684  0.015676467   \n",
      "5  0.021578299  0.012941066   0.01480094  0.0146141965  0.023895685   \n",
      "6   0.06167952   0.02988749   0.06570932   0.031503115   0.30514166   \n",
      "7  0.054756008   0.22191048    0.0349132   0.067287125   0.04567225   \n",
      "8   0.02606819  0.052523606    0.8298748   0.011771273  0.010677853   \n",
      "9        0.005        0.005        0.005     0.9691486        0.005   \n",
      "\n",
      "             c9  \n",
      "0   0.030529771  \n",
      "1  0.0060212472  \n",
      "2    0.20123146  \n",
      "3   0.014436141  \n",
      "4   0.009262325  \n",
      "5   0.020934798  \n",
      "6   0.045768335  \n",
      "7    0.25662062  \n",
      "8         0.005  \n",
      "9  0.0055762376  \n",
      "Number of files: 79726\n",
      "Number of predictions: 79726\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "import os\n",
    "\n",
    "import h5py\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "import keras\n",
    "from keras.preprocessing.image import *\n",
    "from keras.optimizers import *\n",
    "from keras.callbacks import *\n",
    "\n",
    "def gen_kaggle_csv(model, X_test, model_image_size, csv_name):\n",
    "    # 对测试数据进行预测\n",
    "    y_pred = model.predict(X_test, verbose=1)\n",
    "    print(y_pred[:3])  # 打印前3个预测结果\n",
    "    \n",
    "    # 对预测结果进行裁剪，确保它们在合理范围内\n",
    "    y_pred = y_pred.clip(min=0.005, max=0.995)\n",
    "    print()\n",
    "    print(y_pred[:3])  # 打印裁剪后的前3个预测结果\n",
    "\n",
    "    # 创建图像数据生成器\n",
    "    gen = ImageDataGenerator()\n",
    "\n",
    "    # 使用 flow_from_directory 加载测试数据集，不打乱顺序，设置目标大小\n",
    "    test_generator = gen.flow_from_directory(\n",
    "        dir + \"/test/\",  # 测试集文件夹路径\n",
    "        (model_image_size, model_image_size),  # 调整图像大小\n",
    "        shuffle=False,  # 不打乱图像顺序\n",
    "        batch_size=16,  # 设置每批处理的图像数量\n",
    "        class_mode=None  # 因为没有标签，设置为 None\n",
    "    )\n",
    "\n",
    "    # 创建一个空列表来存储文件名和预测值\n",
    "    l = list()\n",
    "\n",
    "    # 遍历测试集的文件名，提取图片名称并与预测结果结合\n",
    "    for i, fname in enumerate(test_generator.filenames):\n",
    "        name = os.path.basename(fname)  # 获取文件名，不包含路径部分\n",
    "        l.append([name, *y_pred[i]])\n",
    "\n",
    "    # 将列表转换为 NumPy 数组\n",
    "    l = np.array(l)\n",
    "\n",
    "    # 创建字典，存储图片名称和预测概率\n",
    "    data = {'img': l[:, 0]}  # 图片名称\n",
    "    for i in range(10):\n",
    "        data[\"c%d\" % i] = l[:, i + 1]  # 每个类别的预测概率\n",
    "\n",
    "    # 将字典转换为 DataFrame，列名是图片名称和类别预测概率\n",
    "    df = pd.DataFrame(data, columns=['img'] + ['c%d' % i for i in range(10)])\n",
    "\n",
    "    # 打印前10行数据检查\n",
    "    print(df.head(10))\n",
    "\n",
    "    # 按照图片名称排序\n",
    "    df = df.sort_values(by='img')\n",
    "\n",
    "    # 将 DataFrame 保存为 CSV 文件\n",
    "    df.to_csv(csv_name, index=None, float_format='%.3f')\n",
    "\n",
    "    # 打印测试集中图片数量和预测结果数量，进行简单的检查\n",
    "    print(f\"Number of files: {len(test_generator.filenames)}\")\n",
    "    print(f\"Number of predictions: {len(y_pred)}\")\n",
    "\n",
    "# 打印完成信息\n",
    "print(\"done\")\n",
    "\n",
    "from tensorflow.keras.models import load_model\n",
    "# 加载已保存的模型\n",
    "model_mix = load_model(\"models/mixed-model.h5\")\n",
    "# 现在你可以使用 model_mix 来进行预测或其他操作\n",
    "\n",
    "dir = \"F:\\JupyterWorkSpace\\BBBBBBS\"\n",
    "\n",
    "test_premodels = [\n",
    "     \"bottleneck_ResNet50_test.h5\", \n",
    "     \"bottleneck_Xception_test.h5\", \n",
    "     \"bottleneck_InceptionV3_test.h5\",\n",
    "]\n",
    "X_test = []\n",
    "for filename in test_premodels:\n",
    "    print('------------------'+filename)\n",
    "    with h5py.File(os.path.join(\"models\", \"finetune\", filename), 'r') as h:\n",
    "        X_test.append(np.array(h['test']))\n",
    "        \n",
    "X_test = np.concatenate(X_test, axis=1)\n",
    "gen_kaggle_csv(model_mix, X_test,  320, 'csv/mixed-pred-test.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 规范代码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------ bottleneck_ResNet50_test.h5\n",
      "------------------ bottleneck_Xception_test.h5\n",
      "------------------ bottleneck_InceptionV3_test.h5\n",
      "2492/2492 [==============================] - 6s 2ms/step\n",
      "Predictions (first 3): [[0.0063405  0.01604431 0.02785161 0.02301869 0.03530462 0.8023355\n",
      "  0.02995177 0.01099865 0.01762458 0.03052977]\n",
      " [0.02174795 0.01262218 0.02817013 0.01803399 0.02463043 0.81699604\n",
      "  0.01878481 0.00956798 0.04342525 0.00602125]\n",
      " [0.16492495 0.07910203 0.05841956 0.06612758 0.09217478 0.03954029\n",
      "  0.07231129 0.11545973 0.11070835 0.20123146]]\n",
      "Clipped Predictions (first 3): [[0.0063405  0.01604431 0.02785161 0.02301869 0.03530462 0.8023355\n",
      "  0.02995177 0.01099865 0.01762458 0.03052977]\n",
      " [0.02174795 0.01262218 0.02817013 0.01803399 0.02463043 0.81699604\n",
      "  0.01878481 0.00956798 0.04342525 0.00602125]\n",
      " [0.16492495 0.07910203 0.05841956 0.06612758 0.09217478 0.03954029\n",
      "  0.07231129 0.11545973 0.11070835 0.20123146]]\n",
      "Found 79726 images belonging to 1 classes.\n",
      "              img           c0           c1           c2           c3  \\\n",
      "0       img_1.jpg  0.006340498  0.016044306  0.027851613  0.023018694   \n",
      "1      img_10.jpg  0.021747952  0.012622183  0.028170126  0.018033987   \n",
      "2     img_100.jpg   0.16492495   0.07910203  0.058419555   0.06612758   \n",
      "3    img_1000.jpg  0.007711893  0.021611858   0.04145989   0.01472676   \n",
      "4  img_100000.jpg  0.038632102  0.035372384  0.021344135   0.75854206   \n",
      "5  img_100001.jpg        0.005  0.024344582  0.008840434   0.85518587   \n",
      "6  img_100002.jpg        0.005    0.2963664  0.016996251   0.14364792   \n",
      "7  img_100003.jpg   0.11359701   0.11046195   0.04238068  0.052400645   \n",
      "8  img_100004.jpg        0.005  0.013597043  0.033391807  0.016658684   \n",
      "9  img_100005.jpg        0.005  0.008914292        0.005        0.005   \n",
      "\n",
      "            c4           c5           c6            c7           c8  \\\n",
      "0   0.03530462    0.8023355  0.029951766   0.010998654  0.017624576   \n",
      "1  0.024630425   0.81699604  0.018784812    0.00956798   0.04342525   \n",
      "2  0.092174776  0.039540287   0.07231129   0.115459725   0.11070835   \n",
      "3  0.026342915  0.016970841  0.021685371   0.021484131   0.81357014   \n",
      "4  0.054481525    0.0244807  0.018291352    0.02391684  0.015676467   \n",
      "5  0.021578299  0.012941066   0.01480094  0.0146141965  0.023895685   \n",
      "6   0.06167952   0.02988749   0.06570932   0.031503115   0.30514166   \n",
      "7  0.054756008   0.22191048    0.0349132   0.067287125   0.04567225   \n",
      "8   0.02606819  0.052523606    0.8298748   0.011771273  0.010677853   \n",
      "9        0.005        0.005        0.005     0.9691486        0.005   \n",
      "\n",
      "             c9  \n",
      "0   0.030529771  \n",
      "1  0.0060212472  \n",
      "2    0.20123146  \n",
      "3   0.014436141  \n",
      "4   0.009262325  \n",
      "5   0.020934798  \n",
      "6   0.045768335  \n",
      "7    0.25662062  \n",
      "8         0.005  \n",
      "9  0.0055762376  \n",
      "Number of files: 79726\n",
      "Number of predictions: 79726\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import h5py\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "def gen_kaggle_csv(model, X_test, model_image_size, csv_name):\n",
    "    \"\"\"\n",
    "    Generate Kaggle submission CSV from model predictions on the test set.\n",
    "    \n",
    "    Args:\n",
    "        model: Trained Keras model.\n",
    "        X_test: Test data for predictions.\n",
    "        model_image_size: Target size for images.\n",
    "        csv_name: Output CSV file name.\n",
    "    \"\"\"\n",
    "    # 对测试数据进行预测\n",
    "    y_pred = model.predict(X_test, verbose=1)\n",
    "    print(\"Predictions (first 3):\", y_pred[:3])  # 打印前3个预测结果\n",
    "    \n",
    "    # 裁剪预测值，确保它们在合理范围内\n",
    "    y_pred = y_pred.clip(min=0.005, max=0.995)\n",
    "    print(\"Clipped Predictions (first 3):\", y_pred[:3])  # 打印裁剪后的前3个预测结果\n",
    "\n",
    "    # 创建图像数据生成器\n",
    "    gen = ImageDataGenerator()\n",
    "\n",
    "    # 使用 flow_from_directory 加载测试数据集\n",
    "    test_generator = gen.flow_from_directory(\n",
    "        dir + \"/test/\",  # 测试集文件夹路径\n",
    "        target_size=(model_image_size, model_image_size),  # 调整图像大小\n",
    "        shuffle=False,  # 不打乱图像顺序\n",
    "        batch_size=16,  # 每批处理的图像数量\n",
    "        class_mode=None  # 没有标签\n",
    "    )\n",
    "\n",
    "    # 存储文件名和预测值的列表\n",
    "    result_list = []\n",
    "    \n",
    "    # 遍历测试集文件名并与预测结果结合\n",
    "    for i, fname in enumerate(test_generator.filenames):\n",
    "        name = os.path.basename(fname)  # 获取文件名（去掉路径部分）\n",
    "        result_list.append([name, *y_pred[i]])\n",
    "\n",
    "    # 转换为 NumPy 数组\n",
    "    result_array = np.array(result_list)\n",
    "\n",
    "    # 创建字典，存储图片名称和预测概率\n",
    "    data = {'img': result_array[:, 0]}  # 图片名称\n",
    "    for i in range(10):\n",
    "        data[f\"c{i}\"] = result_array[:, i + 1]  # 每个类别的预测概率\n",
    "\n",
    "    # 转换为 DataFrame\n",
    "    df = pd.DataFrame(data, columns=['img'] + [f'c{i}' for i in range(10)])\n",
    "\n",
    "    # 打印前10行数据检查\n",
    "    print(df.head(10))\n",
    "\n",
    "    # 按照图片名称排序\n",
    "    df = df.sort_values(by='img')\n",
    "\n",
    "    # 将 DataFrame 保存为 CSV 文件\n",
    "    df.to_csv(csv_name, index=False, float_format='%.3f')\n",
    "\n",
    "    # 输出文件数量和预测结果数量的简单检查\n",
    "    print(f\"Number of files: {len(test_generator.filenames)}\")\n",
    "    print(f\"Number of predictions: {len(y_pred)}\")\n",
    "\n",
    "# 加载已保存的模型\n",
    "model_mix = load_model(\"models/mixed-model.h5\")\n",
    "\n",
    "# 测试数据文件路径\n",
    "dir = \"F:/JupyterWorkSpace/BBBBBBS\"\n",
    "\n",
    "# 预训练模型列表\n",
    "test_premodels = [\n",
    "    \"bottleneck_ResNet50_test.h5\", \n",
    "    \"bottleneck_Xception_test.h5\", \n",
    "    \"bottleneck_InceptionV3_test.h5\",\n",
    "]\n",
    "\n",
    "# 得到混合输入\n",
    "X_test = []\n",
    "for filename in test_premodels:\n",
    "    print(f\"------------------ {filename}\")\n",
    "    with h5py.File(os.path.join(\"models\", \"finetune\", filename), 'r') as h:\n",
    "        X_test.append(np.array(h['test']))\n",
    "        \n",
    "# 合并多个模型的测试数据\n",
    "X_test = np.concatenate(X_test, axis=1)\n",
    "\n",
    "# 生成Kaggle CSV\n",
    "gen_kaggle_csv(model_mix, X_test, 320, 'csv/mixed-pred-test.csv')\n",
    "\n",
    "# 打印完成信息\n",
    "print(\"done\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 单个预测"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 指定图片路径\n",
    "image_path = 'F:\\\\JupyterWorkSpace\\\\BBBBBBS\\\\train\\\\c1\\\\img_6.jpg'  # 请确保这条路径是有效的"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:5 out of the last 5 calls to <function Model.make_predict_function.<locals>.predict_function at 0x0000017061F06DD0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "Prediction: [[0.01973563 0.69541645 0.0319957  0.04667007 0.0370991  0.03033314\n",
      "  0.06915937 0.02763282 0.02671098 0.01524675]]\n",
      "Clipped Prediction: [[0.01973563 0.69541645 0.0319957  0.04667007 0.0370991  0.03033314\n",
      "  0.06915937 0.02763282 0.02671098 0.01524675]]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
    "import h5py\n",
    "\n",
    "def extract_features_from_models(test_premodels, image_path, model_image_size):\n",
    "    \"\"\"\n",
    "    Extract features for a single image from multiple pre-trained models.\n",
    "    \n",
    "    Args:\n",
    "        test_premodels: List of pre-trained model file paths.\n",
    "        image_path: Path to the image for prediction.\n",
    "        model_image_size: Target size for image resizing.\n",
    "    \n",
    "    Returns:\n",
    "        X_test: Extracted features from all pre-trained models.\n",
    "    \"\"\"\n",
    "    X_test = []\n",
    "    \n",
    "    # 加载并预处理图像\n",
    "    img = load_img(image_path, target_size=(model_image_size, model_image_size))\n",
    "    img_array = img_to_array(img)\n",
    "    img_array = np.expand_dims(img_array, axis=0)  # 增加批量维度\n",
    "\n",
    "    # 从每个预训练模型中提取特征\n",
    "    for model_file in test_premodels:\n",
    "        with h5py.File(model_file, 'r') as h:\n",
    "            features = np.array(h['test'])  # 提取预训练模型的特征\n",
    "            X_test.append(features)\n",
    "\n",
    "    # 合并所有特征\n",
    "    X_test = np.concatenate(X_test, axis=1)\n",
    "    return X_test\n",
    "\n",
    "def predict_with_mixed_model(model, X_test):\n",
    "    \"\"\"\n",
    "    Predict the class of a single image using the mixed model and extracted features.\n",
    "    \n",
    "    Args:\n",
    "        model: Trained mixed model.\n",
    "        X_test: Features extracted from pre-trained models.\n",
    "    \"\"\"\n",
    "    # 使用混合模型进行预测\n",
    "    y_pred = model.predict(X_test)\n",
    "    print(f\"Prediction: {y_pred}\")\n",
    "    \n",
    "    # 裁剪预测结果，确保在合理范围内\n",
    "    y_pred = y_pred.clip(min=0.005, max=0.995)\n",
    "    print(f\"Clipped Prediction: {y_pred}\")\n",
    "\n",
    "# 加载混合模型\n",
    "model_mix = load_model(\"models/mixed-model.h5\")\n",
    "\n",
    "# 预训练模型文件路径\n",
    "test_premodels = [\n",
    "    \"models/Single/bottleneck_ResNet50_test.h5\",\n",
    "    \"models/Single/bottleneck_Xception_test.h5\",\n",
    "    \"models/Single/bottleneck_InceptionV3_test.h5\"\n",
    "]\n",
    "\n",
    "\n",
    "# 从预训练模型中提取特征\n",
    "X_test = extract_features_from_models(test_premodels, image_path, 320)\n",
    "\n",
    "# 使用混合模型进行预测\n",
    "predict_with_mixed_model(model_mix, X_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 上面的部分已经好了，说明错误的地方是bottleneck_ResNet50_test等文件的生成\n",
    "下面进行修改"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "本来的，不要运行！！！！！！！！！！！！！！还是这里有问题，需要改\n",
    "\"\"\"\n",
    "def write_gap_test(tag, MODEL, weight_file, image_size, lambda_func=None, featurewise_std_normalization=True):\n",
    "    # 输入张量\n",
    "    input_tensor = Input((*image_size, 3))\n",
    "    x = input_tensor\n",
    "    if lambda_func:\n",
    "        x = Lambda(lambda_func)(x)  # 使用lambda函数进行额外的预处理\n",
    "    # 加载基本模型，不包括顶部的全连接层\n",
    "    base_model = MODEL(input_tensor=x, weights=None, include_top=False)\n",
    "    # 构建最终的模型，使用全局平均池化\n",
    "    model = Model(base_model.input, GlobalAveragePooling2D()(base_model.output))\n",
    "    # 加载权重文件\n",
    "    model.load_weights(\"models/\" + weight_file, by_name=True)\n",
    "\n",
    "    print(MODEL.__name__)\n",
    "    \"\"\"\n",
    "    应该是要从这开始改（下面）\n",
    "    或者，直接把test文件夹里面就放一张数据试试？\n",
    "    需要注意，新建test0文件夹，\n",
    "    输出的test.h5不要放一块（改tag，新建tag文件夹）\n",
    "    \"\"\"\n",
    "    # 定义测试数据的生成器\n",
    "    gen = ImageDataGenerator(\n",
    "        featurewise_std_normalization=featurewise_std_normalization,\n",
    "        samplewise_std_normalization=False,\n",
    "    )\n",
    "    \n",
    "    batch_size = 64\n",
    "    # 加载测试数据\n",
    "    test_generator = gen.flow_from_directory(\n",
    "        os.path.join(dir, 'test'),\n",
    "        target_size=image_size,\n",
    "        shuffle=False,\n",
    "        batch_size=batch_size,\n",
    "        class_mode=None  # 不使用标签\n",
    "    )\n",
    "    \n",
    "    print(\"predict_generator test {}\".format(math.ceil(test_generator.samples // batch_size + 1)))\n",
    "    \n",
    "    # 预测测试数据\n",
    "    test = model.predict(test_generator, steps=math.ceil(test_generator.samples // batch_size + 1))\n",
    "    print(\"test: {}\".format(test.shape))\n",
    "\n",
    "    # 将预测结果保存到文件\n",
    "    print(\"begin create database {}\".format(MODEL.__name__))\n",
    "    with h5py.File(os.path.join(\"models\", tag, \"bottleneck_{}_test.h5\".format(MODEL.__name__)),'w') as h:\n",
    "        h.create_dataset(\"test\", data=test)\n",
    "    \n",
    "    print(\"write_gap_test {} succeeded\".format(MODEL.__name__))\n",
    "    \n",
    "resnet50_weight_file = \"resnet50-imagenet-finetune152.h5\"\n",
    "xception_weight_file = \"xception-imagenet-finetune116.h5\"\n",
    "inceptionV3_weight_file = \"inceptionV3-imagenet-finetune172.h5\"\n",
    "print(\"===== Test =====\")\n",
    "write_gap_test(\"finetune\", ResNet50, resnet50_weight_file, (240, 320))\n",
    "write_gap_test(\"finetune\", Xception, xception_weight_file, (320, 480), xception_preprocess_input)\n",
    "write_gap_test(\"finetune\", InceptionV3, inceptionV3_weight_file, (320, 480), inception_v3_preprocess_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== Test =====\n",
      "Predicting for image: F:/JupyterWorkSpace/BBBBBBS/train/c1/img_115.jpg\n",
      "1/1 [==============================] - 1s 680ms/step\n",
      "Prediction result: (1, 2048)\n",
      "Begin creating database for ResNet50\n",
      "write_gap_test ResNet50 succeeded\n",
      "Predicting for image: F:/JupyterWorkSpace/BBBBBBS/train/c1/img_115.jpg\n",
      "1/1 [==============================] - 1s 570ms/step\n",
      "Prediction result: (1, 2048)\n",
      "Begin creating database for Xception\n",
      "write_gap_test Xception succeeded\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Model, load_model\n",
    "from keras.layers import Input, Lambda, GlobalAveragePooling2D\n",
    "from keras.applications import ResNet50, Xception, InceptionV3\n",
    "from keras.preprocessing import image\n",
    "import numpy as np\n",
    "import h5py\n",
    "import os\n",
    "\n",
    "from tensorflow.keras.preprocessing import image  # 使用 tensorflow.keras\n",
    "\n",
    "from keras.applications.inception_v3 import InceptionV3, preprocess_input as inception_v3_preprocess_input\n",
    "from keras.applications.xception import Xception, preprocess_input as xception_preprocess_input\n",
    "\n",
    "def load_custom_model(MODEL, weight_file, image_size):\n",
    "    # 输入张量\n",
    "    input_tensor = Input((*image_size, 3))\n",
    "    base_model = MODEL(input_tensor=input_tensor, weights=None, include_top=False)\n",
    "    \n",
    "    # 加载权重\n",
    "    base_model.load_weights(\"models/\" + weight_file, by_name=True)\n",
    "    \n",
    "    # 构建最终模型\n",
    "    model = Model(base_model.input, GlobalAveragePooling2D()(base_model.output))\n",
    "    \n",
    "    return model\n",
    "\n",
    "def write_gap_test(tag, MODEL, weight_file, image_size, image_path, lambda_func=None, featurewise_std_normalization=True):\n",
    "    # 检查图片路径是否有效\n",
    "    if not os.path.exists(image_path):\n",
    "        print(f\"Image not found: {image_path}\")\n",
    "        return\n",
    "    \n",
    "    # 加载自定义模型\n",
    "    model = load_custom_model(MODEL, weight_file, image_size)\n",
    "    \n",
    "    print(f\"Predicting for image: {image_path}\")\n",
    "    \n",
    "    # 直接加载和预处理单张图片\n",
    "    img = image.load_img(image_path, target_size=image_size)\n",
    "    img_array = image.img_to_array(img)\n",
    "    img_array = np.expand_dims(img_array, axis=0)  # 增加batch维度\n",
    "    \n",
    "    # 使用lambda函数进行预处理\n",
    "    if lambda_func:\n",
    "        img_array = lambda_func(img_array)\n",
    "    elif featurewise_std_normalization:\n",
    "        img_array = img_array / 255.0  # 假设图片值已归一化\n",
    "    \n",
    "    # 进行预测\n",
    "    test = model.predict(img_array)\n",
    "    print(\"Prediction result: {}\".format(test.shape))\n",
    "\n",
    "    # 将预测结果保存到文件\n",
    "    output_dir = os.path.join(\"models\", tag)\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "\n",
    "    print(f\"Begin creating database for {MODEL.__name__}\")\n",
    "    with h5py.File(os.path.join(output_dir, f\"bottleneck_{MODEL.__name__}_test.h5\"), 'w') as h:\n",
    "        h.create_dataset(\"test\", data=test)\n",
    "    \n",
    "    print(f\"write_gap_test {MODEL.__name__} succeeded\")\n",
    "\n",
    "# 测试不同模型时指定的图片路径\n",
    "image_path = 'F:/JupyterWorkSpace/BBBBBBS/train/c1/img_115.jpg'  # 确保这条路径是有效的\n",
    "\n",
    "resnet50_weight_file = \"resnet50-imagenet-finetune152.h5\"\n",
    "xception_weight_file = \"xception-imagenet-finetune116.h5\"\n",
    "inceptionV3_weight_file = \"inceptionV3-imagenet-finetune172.h5\"\n",
    "\n",
    "print(\"===== Test =====\")\n",
    "write_gap_test(\"Single\", ResNet50, resnet50_weight_file, (240, 320), image_path)\n",
    "write_gap_test(\"Single\", Xception, xception_weight_file, (320, 480), image_path, xception_preprocess_input)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting for image: F:/JupyterWorkSpace/BBBBBBS/train/c1/img_115.jpg\n",
      "1/1 [==============================] - 1s 825ms/step\n",
      "Prediction result: (1, 2048)\n",
      "Begin creating database for InceptionV3\n",
      "write_gap_test InceptionV3 succeeded\n"
     ]
    }
   ],
   "source": [
    "# 需要运行两次\n",
    "write_gap_test(\"Single\", InceptionV3, inceptionV3_weight_file, (320, 480), image_path, inception_v3_preprocess_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  单个预测，汇总"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== Test =====\n",
      "Predicting for image: F:/JupyterWorkSpace/BBBBBBS/train/c1/img_115.jpg\n",
      "1/1 [==============================] - 1s 649ms/step\n",
      "Prediction result: (1, 2048)\n",
      "Begin creating database for ResNet50\n",
      "write_gap_test ResNet50 succeeded\n",
      "Predicting for image: F:/JupyterWorkSpace/BBBBBBS/train/c1/img_115.jpg\n",
      "1/1 [==============================] - 1s 610ms/step\n",
      "Prediction result: (1, 2048)\n",
      "Begin creating database for Xception\n",
      "write_gap_test Xception succeeded\n",
      "Predicting for image: F:/JupyterWorkSpace/BBBBBBS/train/c1/img_115.jpg\n",
      "1/1 [==============================] - 1s 849ms/step\n",
      "Prediction result: (1, 2048)\n",
      "Begin creating database for InceptionV3\n",
      "write_gap_test InceptionV3 succeeded\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "Prediction: [[0.10817979 0.13617077 0.12071446 0.14535302 0.12824251 0.07051165\n",
      "  0.06610069 0.07514649 0.08422078 0.06535983]]\n",
      "Clipped Prediction: [[0.10817979 0.13617077 0.12071446 0.14535302 0.12824251 0.07051165\n",
      "  0.06610069 0.07514649 0.08422078 0.06535983]]\n"
     ]
    }
   ],
   "source": [
    "# 指定图片路径\n",
    "image_path = 'F:\\\\JupyterWorkSpace\\\\BBBBBBS\\\\train\\\\c1\\\\img_6.jpg'  # 请确保这条路径是有效的\n",
    "# image_path = 'F:/JupyterWorkSpace/BBBBBBS/train/c1/img_115.jpg'  # 确保这条路径是有效的\n",
    "\n",
    "from keras.models import Model, load_model\n",
    "from keras.layers import Input, Lambda, GlobalAveragePooling2D\n",
    "from keras.applications import ResNet50, Xception, InceptionV3\n",
    "from keras.preprocessing import image\n",
    "import numpy as np\n",
    "import h5py\n",
    "import os\n",
    "\n",
    "from tensorflow.keras.preprocessing import image  # 使用 tensorflow.keras\n",
    "\n",
    "from keras.applications.inception_v3 import InceptionV3, preprocess_input as inception_v3_preprocess_input\n",
    "from keras.applications.xception import Xception, preprocess_input as xception_preprocess_input\n",
    "\n",
    "def load_custom_model(MODEL, weight_file, image_size):\n",
    "    # 输入张量\n",
    "    input_tensor = Input((*image_size, 3))\n",
    "    base_model = MODEL(input_tensor=input_tensor, weights=None, include_top=False)\n",
    "    \n",
    "    # 加载权重\n",
    "    base_model.load_weights(\"models/\" + weight_file, by_name=True)\n",
    "    \n",
    "    # 构建最终模型\n",
    "    model = Model(base_model.input, GlobalAveragePooling2D()(base_model.output))\n",
    "    \n",
    "    return model\n",
    "\n",
    "def write_gap_test(tag, MODEL, weight_file, image_size, image_path, lambda_func=None, featurewise_std_normalization=True):\n",
    "    # 检查图片路径是否有效\n",
    "    if not os.path.exists(image_path):\n",
    "        print(f\"Image not found: {image_path}\")\n",
    "        return\n",
    "    \n",
    "    # 加载自定义模型\n",
    "    model = load_custom_model(MODEL, weight_file, image_size)\n",
    "    \n",
    "    print(f\"Predicting for image: {image_path}\")\n",
    "    \n",
    "    # 直接加载和预处理单张图片\n",
    "    img = image.load_img(image_path, target_size=image_size)\n",
    "    img_array = image.img_to_array(img)\n",
    "    img_array = np.expand_dims(img_array, axis=0)  # 增加batch维度\n",
    "    \n",
    "    # 使用lambda函数进行预处理\n",
    "    if lambda_func:\n",
    "        img_array = lambda_func(img_array)\n",
    "    elif featurewise_std_normalization:\n",
    "        img_array = img_array / 255.0  # 假设图片值已归一化\n",
    "    \n",
    "    # 进行预测\n",
    "    test = model.predict(img_array)\n",
    "    print(\"Prediction result: {}\".format(test.shape))\n",
    "\n",
    "    # 将预测结果保存到文件\n",
    "    output_dir = os.path.join(\"models\", tag)\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "\n",
    "    print(f\"Begin creating database for {MODEL.__name__}\")\n",
    "    with h5py.File(os.path.join(output_dir, f\"bottleneck_{MODEL.__name__}_test.h5\"), 'w') as h:\n",
    "        h.create_dataset(\"test\", data=test)\n",
    "    \n",
    "    print(f\"write_gap_test {MODEL.__name__} succeeded\")\n",
    "\n",
    "# 测试不同模型时指定的图片路径\n",
    "image_path = 'F:/JupyterWorkSpace/BBBBBBS/train/c1/img_115.jpg'  # 确保这条路径是有效的\n",
    "\n",
    "resnet50_weight_file = \"resnet50-imagenet-finetune152.h5\"\n",
    "xception_weight_file = \"xception-imagenet-finetune116.h5\"\n",
    "inceptionV3_weight_file = \"inceptionV3-imagenet-finetune172.h5\"\n",
    "\n",
    "print(\"===== Test =====\")\n",
    "write_gap_test(\"Single\", ResNet50, resnet50_weight_file, (240, 320), image_path)\n",
    "write_gap_test(\"Single\", Xception, xception_weight_file, (320, 480), image_path, xception_preprocess_input)\n",
    "# 需要运行两次\n",
    "write_gap_test(\"Single\", InceptionV3, inceptionV3_weight_file, (320, 480), image_path, inception_v3_preprocess_input)\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
    "import h5py\n",
    "\n",
    "def extract_features_from_models(test_premodels, image_path, model_image_size):\n",
    "    \"\"\"\n",
    "    Extract features for a single image from multiple pre-trained models.\n",
    "    \n",
    "    Args:\n",
    "        test_premodels: List of pre-trained model file paths.\n",
    "        image_path: Path to the image for prediction.\n",
    "        model_image_size: Target size for image resizing.\n",
    "    \n",
    "    Returns:\n",
    "        X_test: Extracted features from all pre-trained models.\n",
    "    \"\"\"\n",
    "    X_test = []\n",
    "    \n",
    "    # 加载并预处理图像\n",
    "    img = load_img(image_path, target_size=(model_image_size, model_image_size))\n",
    "    img_array = img_to_array(img)\n",
    "    img_array = np.expand_dims(img_array, axis=0)  # 增加批量维度\n",
    "\n",
    "    # 从每个预训练模型中提取特征\n",
    "    for model_file in test_premodels:\n",
    "        with h5py.File(model_file, 'r') as h:\n",
    "            features = np.array(h['test'])  # 提取预训练模型的特征\n",
    "            X_test.append(features)\n",
    "\n",
    "    # 合并所有特征\n",
    "    X_test = np.concatenate(X_test, axis=1)\n",
    "    return X_test\n",
    "\n",
    "def predict_with_mixed_model(model, X_test):\n",
    "    \"\"\"\n",
    "    Predict the class of a single image using the mixed model and extracted features.\n",
    "    \n",
    "    Args:\n",
    "        model: Trained mixed model.\n",
    "        X_test: Features extracted from pre-trained models.\n",
    "    \"\"\"\n",
    "    # 使用混合模型进行预测\n",
    "    y_pred = model.predict(X_test)\n",
    "    print(f\"Prediction: {y_pred}\")\n",
    "    \n",
    "    # 裁剪预测结果，确保在合理范围内\n",
    "    y_pred = y_pred.clip(min=0.005, max=0.995)\n",
    "    print(f\"Clipped Prediction: {y_pred}\")\n",
    "\n",
    "# 加载混合模型\n",
    "model_mix = load_model(\"models/mixed-model.h5\")\n",
    "\n",
    "# 预训练模型文件路径\n",
    "test_premodels = [\n",
    "    \"models/Single/bottleneck_ResNet50_test.h5\",\n",
    "    \"models/Single/bottleneck_Xception_test.h5\",\n",
    "    \"models/Single/bottleneck_InceptionV3_test.h5\"\n",
    "]\n",
    "\n",
    "\n",
    "# 从预训练模型中提取特征\n",
    "X_test = extract_features_from_models(test_premodels, image_path, 320)\n",
    "\n",
    "# 使用混合模型进行预测\n",
    "predict_with_mixed_model(model_mix, X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== Test =====\n",
      "Predicting for image: F:/JupyterWorkSpace/BBBBBBS/train/c1/img_115.jpg\n",
      "1/1 [==============================] - 1s 715ms/step\n",
      "Prediction result: (1, 2048)\n",
      "Saving features for ResNet50\n",
      "write_gap_test for ResNet50 succeeded\n",
      "Predicting for image: F:/JupyterWorkSpace/BBBBBBS/train/c1/img_115.jpg\n",
      "1/1 [==============================] - 1s 582ms/step\n",
      "Prediction result: (1, 2048)\n",
      "Saving features for Xception\n",
      "write_gap_test for Xception succeeded\n",
      "Predicting for image: F:/JupyterWorkSpace/BBBBBBS/train/c1/img_115.jpg\n",
      "1/1 [==============================] - 1s 846ms/step\n",
      "Prediction result: (1, 2048)\n",
      "Saving features for InceptionV3\n",
      "write_gap_test for InceptionV3 succeeded\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "Prediction: [[0.11752235 0.12432297 0.04872757 0.23426199 0.12652394 0.02233721\n",
      "  0.043757   0.0460817  0.0551057  0.18135951]]\n",
      "Clipped Prediction: [[0.11752235 0.12432297 0.04872757 0.23426199 0.12652394 0.02233721\n",
      "  0.043757   0.0460817  0.0551057  0.18135951]]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import h5py\n",
    "from tensorflow.keras.models import Model, load_model\n",
    "from tensorflow.keras.layers import Input, GlobalAveragePooling2D\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from tensorflow.keras.applications import ResNet50, Xception, InceptionV3\n",
    "from tensorflow.keras.applications.inception_v3 import preprocess_input as inception_v3_preprocess_input\n",
    "from tensorflow.keras.applications.xception import preprocess_input as xception_preprocess_input\n",
    "\n",
    "import tensorflow as tf\n",
    "import random\n",
    "np.random.seed(8147)\n",
    "random.seed(8147)\n",
    "tf.random.set_seed(8147)\n",
    "\n",
    "\n",
    "def load_custom_model(model_class, weight_file, image_size, weights_dir=\"models\"):\n",
    "    \"\"\"\n",
    "    Load a custom model with pre-trained weights.\n",
    "    \n",
    "    Args:\n",
    "        model_class: Pre-trained model class (e.g., ResNet50, Xception, etc.).\n",
    "        weight_file: Filename of the pre-trained weights.\n",
    "        image_size: Target image size for the model input.\n",
    "        weights_dir: Directory containing the model weights (default is 'models').\n",
    "    \n",
    "    Returns:\n",
    "        model: The custom model with the pre-trained weights loaded.\n",
    "    \"\"\"\n",
    "    input_tensor = Input((*image_size, 3))\n",
    "    base_model = model_class(input_tensor=input_tensor, weights=None, include_top=False)\n",
    "    \n",
    "    # Load weights\n",
    "    weight_path = os.path.join(weights_dir, weight_file)\n",
    "    if not os.path.exists(weight_path):\n",
    "        raise FileNotFoundError(f\"Weight file not found: {weight_path}\")\n",
    "    \n",
    "    base_model.load_weights(weight_path, by_name=True)\n",
    "    \n",
    "    # Build final model\n",
    "    model = Model(base_model.input, GlobalAveragePooling2D()(base_model.output))\n",
    "    \n",
    "    return model\n",
    "\n",
    "\n",
    "def write_gap_test(tag, model_class, weight_file, image_size, image_path, preprocess_input_func=None, featurewise_std_normalization=True, weights_dir=\"models\"):\n",
    "    \"\"\"\n",
    "    Process a single image and extract features using a custom model, then save the features to an HDF5 file.\n",
    "    \n",
    "    Args:\n",
    "        tag: Tag to organize output directory.\n",
    "        model_class: Pre-trained model class (e.g., ResNet50, Xception, etc.).\n",
    "        weight_file: Filename of the pre-trained weights.\n",
    "        image_size: Target image size for the model input.\n",
    "        image_path: Path to the image to be processed.\n",
    "        preprocess_input_func: Preprocessing function specific to the model.\n",
    "        featurewise_std_normalization: Whether to apply featurewise standard normalization (default is True).\n",
    "        weights_dir: Directory containing the model weights (default is 'models').\n",
    "    \"\"\"\n",
    "    if not os.path.exists(image_path):\n",
    "        print(f\"Image not found: {image_path}\")\n",
    "        return\n",
    "    \n",
    "    model = load_custom_model(model_class, weight_file, image_size, weights_dir)\n",
    "    \n",
    "    print(f\"Predicting for image: {image_path}\")\n",
    "    \n",
    "    # Load and preprocess the image\n",
    "    img = image.load_img(image_path, target_size=image_size)\n",
    "    img_array = image.img_to_array(img)\n",
    "    img_array = np.expand_dims(img_array, axis=0)  # Add batch dimension\n",
    "    \n",
    "    # Apply preprocessing if necessary\n",
    "    if preprocess_input_func:\n",
    "        img_array = preprocess_input_func(img_array)\n",
    "    elif featurewise_std_normalization:\n",
    "        img_array = img_array / 255.0  # Normalize pixel values\n",
    "    \n",
    "    # Predict features\n",
    "    test = model.predict(img_array)\n",
    "    print(\"Prediction result: {}\".format(test.shape))\n",
    "\n",
    "    # Save the features to an HDF5 file\n",
    "    output_dir = os.path.join(\"models\", tag)\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "\n",
    "    print(f\"Saving features for {model_class.__name__}\")\n",
    "    with h5py.File(os.path.join(output_dir, f\"bottleneck_{model_class.__name__}_test.h5\"), 'w') as h:\n",
    "        h.create_dataset(\"test\", data=test)\n",
    "    \n",
    "    print(f\"write_gap_test for {model_class.__name__} succeeded\")\n",
    "\n",
    "\n",
    "def extract_features_from_models(model_files, image_path, model_image_size):\n",
    "    \"\"\"\n",
    "    Extract features from a single image using multiple pre-trained models.\n",
    "    \n",
    "    Args:\n",
    "        model_files: List of HDF5 files containing extracted features from pre-trained models.\n",
    "        image_path: Path to the image to be processed.\n",
    "        model_image_size: Target size for image resizing.\n",
    "    \n",
    "    Returns:\n",
    "        X_test: Combined features extracted from all models.\n",
    "    \"\"\"\n",
    "    X_test = []\n",
    "    \n",
    "    # Load and preprocess the image\n",
    "    img = image.load_img(image_path, target_size=(model_image_size, model_image_size))\n",
    "    img_array = image.img_to_array(img)\n",
    "    img_array = np.expand_dims(img_array, axis=0)  # Add batch dimension\n",
    "\n",
    "    # Extract features from each pre-trained model\n",
    "    for model_file in model_files:\n",
    "        with h5py.File(model_file, 'r') as h:\n",
    "            features = np.array(h['test'])  # Extract features from HDF5 file\n",
    "            X_test.append(features)\n",
    "\n",
    "    # Combine all features\n",
    "    X_test = np.concatenate(X_test, axis=1)\n",
    "    return X_test\n",
    "\n",
    "\n",
    "def predict_with_mixed_model(model, X_test):\n",
    "    \"\"\"\n",
    "    Predict the class of a single image using the mixed model and extracted features.\n",
    "    \n",
    "    Args:\n",
    "        model: Trained mixed model.\n",
    "        X_test: Extracted features from pre-trained models.\n",
    "    \"\"\"\n",
    "    y_pred = model.predict(X_test)\n",
    "    print(f\"Prediction: {y_pred}\")\n",
    "    \n",
    "    # Clip the prediction results to a valid range\n",
    "    y_pred = y_pred.clip(min=0.005, max=0.995)\n",
    "    print(f\"Clipped Prediction: {y_pred}\")\n",
    "\n",
    "\n",
    "# Paths and filenames\n",
    "image_path = 'F:/JupyterWorkSpace/BBBBBBS/train/c1/img_115.jpg'\n",
    "resnet50_weight_file = \"resnet50-imagenet-finetune152.h5\"\n",
    "xception_weight_file = \"xception-imagenet-finetune116.h5\"\n",
    "inceptionV3_weight_file = \"inceptionV3-imagenet-finetune172.h5\"\n",
    "\n",
    "print(\"===== Test =====\")\n",
    "write_gap_test(\"Single\", ResNet50, resnet50_weight_file, (240, 320), image_path)\n",
    "write_gap_test(\"Single\", Xception, xception_weight_file, (320, 480), image_path, xception_preprocess_input)\n",
    "write_gap_test(\"Single\", InceptionV3, inceptionV3_weight_file, (320, 480), image_path, inception_v3_preprocess_input)\n",
    "\n",
    "# Mixed model prediction\n",
    "model_mix = load_model(\"models/mixed-model.h5\")\n",
    "\n",
    "test_premodels = [\n",
    "    \"models/Single/bottleneck_ResNet50_test.h5\",\n",
    "    \"models/Single/bottleneck_Xception_test.h5\",\n",
    "    \"models/Single/bottleneck_InceptionV3_test.h5\"\n",
    "]\n",
    "\n",
    "# Extract features and predict using the mixed model\n",
    "X_test = extract_features_from_models(test_premodels, image_path, 320)\n",
    "predict_with_mixed_model(model_mix, X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 15ms/step\n",
      "Prediction: [[0.08819199 0.09396999 0.12517974 0.12770745 0.15163337 0.07297713\n",
      "  0.08025378 0.08779748 0.08683237 0.08545671]]\n",
      "Clipped Prediction: [[0.08819199 0.09396999 0.12517974 0.12770745 0.15163337 0.07297713\n",
      "  0.08025378 0.08779748 0.08683237 0.08545671]]\n"
     ]
    }
   ],
   "source": [
    "predict_with_mixed_model(model_mix, X_test)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
