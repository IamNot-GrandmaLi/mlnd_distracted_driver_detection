{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "load libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from keras.models import *\n",
    "from keras.optimizers import *\n",
    "from keras.layers import *\n",
    "from keras.applications import *\n",
    "from keras.preprocessing.image import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dir = \"/ext/Data/distracted_driver_detection/\"\n",
    "dir = \"F:\\BBBBBBBBBBBBBBBBB\\Datas\"\n",
    "\n",
    "model_image_size = (240, 360)\n",
    "fine_tune_layer = 152\n",
    "# final_layer = 176\n",
    "final_layer = 177\n",
    "    # BatchNormalization 和Dropout 通常不包含权重，无法通过 .get_weights() 来访问它们；\n",
    "    # 如果你试图提取权重，请确保访问的是包含可训练参数的层，例如卷积层（Conv2D）或全连接层（Dense）。\n",
    "visual_layer = 172\n",
    "batch_size = 128\n",
    "\n",
    "def lambda_func(x):\n",
    "    x /= 255.\n",
    "    x -= 0.5\n",
    "    x *= 2\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# 加载数据集\n",
    "\n",
    "load train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\13904\\anaconda3\\lib\\site-packages\\keras\\preprocessing\\image.py:1460: UserWarning: This ImageDataGenerator specifies `featurewise_std_normalization`, which overrides setting of `featurewise_center`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 20787 images belonging to 10 classes.\n",
      "subdior to train type {'c0': 0, 'c1': 1, 'c2': 2, 'c3': 3, 'c4': 4, 'c5': 5, 'c6': 6, 'c7': 7, 'c8': 8, 'c9': 9}\n",
      "Found 1637 images belonging to 10 classes.\n",
      "subdior to valid type {'c0': 0, 'c1': 1, 'c2': 2, 'c3': 3, 'c4': 4, 'c5': 5, 'c6': 6, 'c7': 7, 'c8': 8, 'c9': 9}\n"
     ]
    }
   ],
   "source": [
    "train_gen = ImageDataGenerator(\n",
    "    featurewise_std_normalization=True,\n",
    "    samplewise_std_normalization=False,\n",
    "    rotation_range=10.,\n",
    "    width_shift_range=0.05,\n",
    "    height_shift_range=0.05,\n",
    "    shear_range=0.1,\n",
    "    zoom_range=0.1,\n",
    ")\n",
    "gen = ImageDataGenerator(\n",
    "    featurewise_std_normalization=True,\n",
    "    samplewise_std_normalization=False,\n",
    ")\n",
    "train_generator = train_gen.flow_from_directory(os.path.join(dir, 'train'),  model_image_size, shuffle=True, batch_size=batch_size, class_mode=\"categorical\")\n",
    "print(\"subdior to train type {}\".format(train_generator.class_indices))\n",
    "valid_generator = gen.flow_from_directory(os.path.join(dir, 'valid'),  model_image_size, shuffle=True, batch_size=batch_size, class_mode=\"categorical\")\n",
    "print(\"subdior to valid type {}\".format(valid_generator.class_indices))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 构建模型\n",
    "\n",
    "https://github.com/fchollet/keras/blob/master/keras/applications/resnet50.py\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/resnet/resnet50_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
      "94765736/94765736 [==============================] - 9s 0us/step\n",
      "total layer count 175\n"
     ]
    }
   ],
   "source": [
    "input_tensor = Input((*model_image_size, 3))\n",
    "x = input_tensor\n",
    "# if lambda_func:\n",
    "#     x = Lambda(lambda_func)(x)\n",
    "\n",
    "base_model = ResNet50(input_tensor=Input((*model_image_size, 3)), weights='imagenet', include_top=False)\n",
    "\n",
    "x = GlobalAveragePooling2D()(base_model.output)\n",
    "x = Dropout(0.5)(x)\n",
    "x = Dense(10, activation='softmax')(x)\n",
    "model = Model(base_model.input, x)\n",
    "\n",
    "print(\"total layer count {}\".format(len(base_model.layers)))\n",
    "\n",
    "for i in range(fine_tune_layer):\n",
    "    model.layers[i].trainable = False\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 训练模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_generator.samples = 20787\n",
      "valid_generator.samples = 1637\n"
     ]
    }
   ],
   "source": [
    "print(\"train_generator.samples = {}\".format(train_generator.samples))\n",
    "print(\"valid_generator.samples = {}\".format(valid_generator.samples))\n",
    "steps_train_sample = train_generator.samples // 128 + 1\n",
    "steps_valid_sample = valid_generator.samples // 128 + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\13904\\AppData\\Local\\Temp\\ipykernel_11072\\1649610113.py:2: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
      "  model.fit_generator(train_generator, steps_per_epoch=steps_train_sample, epochs=6, validation_data=valid_generator, validation_steps=steps_valid_sample)\n",
      "C:\\Users\\13904\\anaconda3\\lib\\site-packages\\keras\\preprocessing\\image.py:1861: UserWarning: This ImageDataGenerator specifies `featurewise_center`, but it hasn't been fit on any training data. Fit it first by calling `.fit(numpy_data)`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\13904\\anaconda3\\lib\\site-packages\\keras\\preprocessing\\image.py:1871: UserWarning: This ImageDataGenerator specifies `featurewise_std_normalization`, but it hasn't been fit on any training data. Fit it first by calling `.fit(numpy_data)`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/6\n",
      "163/163 [==============================] - 1527s 9s/step - loss: 0.2107 - accuracy: 0.9339 - val_loss: 1.6385 - val_accuracy: 0.7025\n",
      "Epoch 2/6\n",
      "163/163 [==============================] - 1522s 9s/step - loss: 0.0475 - accuracy: 0.9851 - val_loss: 1.3144 - val_accuracy: 0.7318\n",
      "Epoch 3/6\n",
      "163/163 [==============================] - 1517s 9s/step - loss: 0.0333 - accuracy: 0.9903 - val_loss: 2.2476 - val_accuracy: 0.6213\n",
      "Epoch 4/6\n",
      "163/163 [==============================] - 1518s 9s/step - loss: 0.0217 - accuracy: 0.9935 - val_loss: 1.2544 - val_accuracy: 0.6799\n",
      "Epoch 5/6\n",
      "163/163 [==============================] - 1517s 9s/step - loss: 0.0228 - accuracy: 0.9930 - val_loss: 1.4347 - val_accuracy: 0.7239\n",
      "Epoch 6/6\n",
      "163/163 [==============================] - 1519s 9s/step - loss: 0.0139 - accuracy: 0.9959 - val_loss: 2.0465 - val_accuracy: 0.6420\n",
      "model saved!\n"
     ]
    }
   ],
   "source": [
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "model.fit_generator(train_generator, steps_per_epoch=steps_train_sample, epochs=6, validation_data=valid_generator, validation_steps=steps_valid_sample)\n",
    "\n",
    "model.save(\"models/resnet50-imagenet-finetune{}-adam.h5\".format(fine_tune_layer))\n",
    "print(\"model saved!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\13904\\anaconda3\\lib\\site-packages\\keras\\optimizers\\legacy\\rmsprop.py:143: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n",
      "C:\\Users\\13904\\AppData\\Local\\Temp\\ipykernel_11072\\365326434.py:2: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
      "  model.fit_generator(train_generator, steps_per_epoch=steps_train_sample, epochs=6, validation_data=valid_generator, validation_steps=steps_valid_sample)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/6\n",
      "163/163 [==============================] - 1517s 9s/step - loss: 0.0109 - accuracy: 0.9968 - val_loss: 1.3901 - val_accuracy: 0.6988\n",
      "Epoch 2/6\n",
      "163/163 [==============================] - 1522s 9s/step - loss: 0.0064 - accuracy: 0.9983 - val_loss: 1.3989 - val_accuracy: 0.6982\n",
      "Epoch 3/6\n",
      "163/163 [==============================] - 1520s 9s/step - loss: 0.0045 - accuracy: 0.9987 - val_loss: 1.4210 - val_accuracy: 0.7013\n",
      "Epoch 4/6\n",
      "163/163 [==============================] - 1522s 9s/step - loss: 0.0046 - accuracy: 0.9986 - val_loss: 1.4310 - val_accuracy: 0.7037\n",
      "Epoch 5/6\n",
      "163/163 [==============================] - 1518s 9s/step - loss: 0.0033 - accuracy: 0.9991 - val_loss: 1.5054 - val_accuracy: 0.7019\n",
      "Epoch 6/6\n",
      "163/163 [==============================] - 1518s 9s/step - loss: 0.0026 - accuracy: 0.9992 - val_loss: 1.5541 - val_accuracy: 0.7019\n",
      "model saved!\n"
     ]
    }
   ],
   "source": [
    "model.compile(optimizer=RMSprop(lr=1*0.00001), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "model.fit_generator(train_generator, steps_per_epoch=steps_train_sample, epochs=6, validation_data=valid_generator, validation_steps=steps_valid_sample)\n",
    "\n",
    "model.save(\"models/resnet50-imagenet-finetune{}.h5\".format(fine_tune_layer))\n",
    "print(\"model saved!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 可视化模型\n",
    "\n",
    "https://keras.io/visualization/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load successed\n"
     ]
    }
   ],
   "source": [
    "from IPython.display import SVG\n",
    "from keras.utils.vis_utils import model_to_dot\n",
    "from keras.models import *\n",
    "\n",
    "model = load_model(\"models/resnet50-imagenet-finetune{}.h5\".format(fine_tune_layer))\n",
    "print(\"load successed\")\n",
    "\n",
    "#SVG(model_to_dot(model).create(prog='dot', format='svg'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CAM 可视化\n",
    "\n",
    "http://cnnlocalization.csail.mit.edu/\n",
    "\n",
    "![](http://cnnlocalization.csail.mit.edu/framework.jpg)\n",
    "\n",
    "$cam = (P-0.5)*w*output$\n",
    "\n",
    "* cam: 类激活图 X\\*X\n",
    "* P: 概率\n",
    "* output: 卷积层的输出 2048\\*1\n",
    "* w: 卷积核的权重 X\\*X\\*2048"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_2 - 0\n",
      "conv1_pad - 1\n",
      "conv1_conv - 2\n",
      "conv1_bn - 3\n",
      "conv1_relu - 4\n",
      "pool1_pad - 5\n",
      "pool1_pool - 6\n",
      "conv2_block1_1_conv - 7\n",
      "conv2_block1_1_bn - 8\n",
      "conv2_block1_1_relu - 9\n",
      "conv2_block1_2_conv - 10\n",
      "conv2_block1_2_bn - 11\n",
      "conv2_block1_2_relu - 12\n",
      "conv2_block1_0_conv - 13\n",
      "conv2_block1_3_conv - 14\n",
      "conv2_block1_0_bn - 15\n",
      "conv2_block1_3_bn - 16\n",
      "conv2_block1_add - 17\n",
      "conv2_block1_out - 18\n",
      "conv2_block2_1_conv - 19\n",
      "conv2_block2_1_bn - 20\n",
      "conv2_block2_1_relu - 21\n",
      "conv2_block2_2_conv - 22\n",
      "conv2_block2_2_bn - 23\n",
      "conv2_block2_2_relu - 24\n",
      "conv2_block2_3_conv - 25\n",
      "conv2_block2_3_bn - 26\n",
      "conv2_block2_add - 27\n",
      "conv2_block2_out - 28\n",
      "conv2_block3_1_conv - 29\n",
      "conv2_block3_1_bn - 30\n",
      "conv2_block3_1_relu - 31\n",
      "conv2_block3_2_conv - 32\n",
      "conv2_block3_2_bn - 33\n",
      "conv2_block3_2_relu - 34\n",
      "conv2_block3_3_conv - 35\n",
      "conv2_block3_3_bn - 36\n",
      "conv2_block3_add - 37\n",
      "conv2_block3_out - 38\n",
      "conv3_block1_1_conv - 39\n",
      "conv3_block1_1_bn - 40\n",
      "conv3_block1_1_relu - 41\n",
      "conv3_block1_2_conv - 42\n",
      "conv3_block1_2_bn - 43\n",
      "conv3_block1_2_relu - 44\n",
      "conv3_block1_0_conv - 45\n",
      "conv3_block1_3_conv - 46\n",
      "conv3_block1_0_bn - 47\n",
      "conv3_block1_3_bn - 48\n",
      "conv3_block1_add - 49\n",
      "conv3_block1_out - 50\n",
      "conv3_block2_1_conv - 51\n",
      "conv3_block2_1_bn - 52\n",
      "conv3_block2_1_relu - 53\n",
      "conv3_block2_2_conv - 54\n",
      "conv3_block2_2_bn - 55\n",
      "conv3_block2_2_relu - 56\n",
      "conv3_block2_3_conv - 57\n",
      "conv3_block2_3_bn - 58\n",
      "conv3_block2_add - 59\n",
      "conv3_block2_out - 60\n",
      "conv3_block3_1_conv - 61\n",
      "conv3_block3_1_bn - 62\n",
      "conv3_block3_1_relu - 63\n",
      "conv3_block3_2_conv - 64\n",
      "conv3_block3_2_bn - 65\n",
      "conv3_block3_2_relu - 66\n",
      "conv3_block3_3_conv - 67\n",
      "conv3_block3_3_bn - 68\n",
      "conv3_block3_add - 69\n",
      "conv3_block3_out - 70\n",
      "conv3_block4_1_conv - 71\n",
      "conv3_block4_1_bn - 72\n",
      "conv3_block4_1_relu - 73\n",
      "conv3_block4_2_conv - 74\n",
      "conv3_block4_2_bn - 75\n",
      "conv3_block4_2_relu - 76\n",
      "conv3_block4_3_conv - 77\n",
      "conv3_block4_3_bn - 78\n",
      "conv3_block4_add - 79\n",
      "conv3_block4_out - 80\n",
      "conv4_block1_1_conv - 81\n",
      "conv4_block1_1_bn - 82\n",
      "conv4_block1_1_relu - 83\n",
      "conv4_block1_2_conv - 84\n",
      "conv4_block1_2_bn - 85\n",
      "conv4_block1_2_relu - 86\n",
      "conv4_block1_0_conv - 87\n",
      "conv4_block1_3_conv - 88\n",
      "conv4_block1_0_bn - 89\n",
      "conv4_block1_3_bn - 90\n",
      "conv4_block1_add - 91\n",
      "conv4_block1_out - 92\n",
      "conv4_block2_1_conv - 93\n",
      "conv4_block2_1_bn - 94\n",
      "conv4_block2_1_relu - 95\n",
      "conv4_block2_2_conv - 96\n",
      "conv4_block2_2_bn - 97\n",
      "conv4_block2_2_relu - 98\n",
      "conv4_block2_3_conv - 99\n",
      "conv4_block2_3_bn - 100\n",
      "conv4_block2_add - 101\n",
      "conv4_block2_out - 102\n",
      "conv4_block3_1_conv - 103\n",
      "conv4_block3_1_bn - 104\n",
      "conv4_block3_1_relu - 105\n",
      "conv4_block3_2_conv - 106\n",
      "conv4_block3_2_bn - 107\n",
      "conv4_block3_2_relu - 108\n",
      "conv4_block3_3_conv - 109\n",
      "conv4_block3_3_bn - 110\n",
      "conv4_block3_add - 111\n",
      "conv4_block3_out - 112\n",
      "conv4_block4_1_conv - 113\n",
      "conv4_block4_1_bn - 114\n",
      "conv4_block4_1_relu - 115\n",
      "conv4_block4_2_conv - 116\n",
      "conv4_block4_2_bn - 117\n",
      "conv4_block4_2_relu - 118\n",
      "conv4_block4_3_conv - 119\n",
      "conv4_block4_3_bn - 120\n",
      "conv4_block4_add - 121\n",
      "conv4_block4_out - 122\n",
      "conv4_block5_1_conv - 123\n",
      "conv4_block5_1_bn - 124\n",
      "conv4_block5_1_relu - 125\n",
      "conv4_block5_2_conv - 126\n",
      "conv4_block5_2_bn - 127\n",
      "conv4_block5_2_relu - 128\n",
      "conv4_block5_3_conv - 129\n",
      "conv4_block5_3_bn - 130\n",
      "conv4_block5_add - 131\n",
      "conv4_block5_out - 132\n",
      "conv4_block6_1_conv - 133\n",
      "conv4_block6_1_bn - 134\n",
      "conv4_block6_1_relu - 135\n",
      "conv4_block6_2_conv - 136\n",
      "conv4_block6_2_bn - 137\n",
      "conv4_block6_2_relu - 138\n",
      "conv4_block6_3_conv - 139\n",
      "conv4_block6_3_bn - 140\n",
      "conv4_block6_add - 141\n",
      "conv4_block6_out - 142\n",
      "conv5_block1_1_conv - 143\n",
      "conv5_block1_1_bn - 144\n",
      "conv5_block1_1_relu - 145\n",
      "conv5_block1_2_conv - 146\n",
      "conv5_block1_2_bn - 147\n",
      "conv5_block1_2_relu - 148\n",
      "conv5_block1_0_conv - 149\n",
      "conv5_block1_3_conv - 150\n",
      "conv5_block1_0_bn - 151\n",
      "conv5_block1_3_bn - 152\n",
      "conv5_block1_add - 153\n",
      "conv5_block1_out - 154\n",
      "conv5_block2_1_conv - 155\n",
      "conv5_block2_1_bn - 156\n",
      "conv5_block2_1_relu - 157\n",
      "conv5_block2_2_conv - 158\n",
      "conv5_block2_2_bn - 159\n",
      "conv5_block2_2_relu - 160\n",
      "conv5_block2_3_conv - 161\n",
      "conv5_block2_3_bn - 162\n",
      "conv5_block2_add - 163\n",
      "conv5_block2_out - 164\n",
      "conv5_block3_1_conv - 165\n",
      "conv5_block3_1_bn - 166\n",
      "conv5_block3_1_relu - 167\n",
      "conv5_block3_2_conv - 168\n",
      "conv5_block3_2_bn - 169\n",
      "conv5_block3_2_relu - 170\n",
      "conv5_block3_3_conv - 171\n",
      "conv5_block3_3_bn - 172\n",
      "conv5_block3_add - 173\n",
      "conv5_block3_out - 174\n",
      "global_average_pooling2d - 175\n",
      "dropout - 176\n",
      "dense - 177\n"
     ]
    }
   ],
   "source": [
    "z = zip([x.name for x in model.layers], range(len(model.layers)))\n",
    "for k, v in z:\n",
    "    print(\"{} - {}\".format(k,v))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "http://docs.opencv.org/trunk/d3/d50/group__imgproc__colormap.html\n",
    "\n",
    "![](http://docs.opencv.org/trunk/colorscale_jet.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "def show_heatmap_image(model_show, weights_show):\n",
    "    test_dir = os.path.join(dir,  \"test\" )\n",
    "    image_files = glob.glob(os.path.join(test_dir,\"*\"))\n",
    "    print(len(image_files))\n",
    "    \n",
    "    plt.figure(figsize=(12, 24))\n",
    "    for i in range(10):\n",
    "        plt.subplot(5, 2, i+1)\n",
    "        img = cv2.imread(image_files[2000*i+113])\n",
    "        img = cv2.resize(img,  (model_image_size[1],model_image_size[0]))\n",
    "        x = img.copy()\n",
    "        x.astype(np.float32)\n",
    "        out, predictions = model_show.predict(np.expand_dims(x, axis=0))\n",
    "        predictions = predictions[0]\n",
    "        out = out[0]\n",
    "        \n",
    "        max_idx = np.argmax(predictions)\n",
    "        prediction = predictions[max_idx]\n",
    "\n",
    "        status = [\"safe driving\",  \" texting - right\",  \"phone - right\",  \"texting - left\",  \"phone - left\",  \n",
    "                  \"operation radio\", \"drinking\", \"reaching behind\", \"hair and makeup\", \"talking\"]\n",
    "\n",
    "        plt.title('c%d |%s| %.2f%%' % (max_idx , status[max_idx], prediction*100))\n",
    "    \n",
    "        cam = (prediction - 0.5) * np.matmul(out, weights_show)\n",
    "        cam = cam[:,:,max_idx]\n",
    "        cam -= cam.min()\n",
    "        cam /= cam.max()\n",
    "        cam -= 0.2\n",
    "        cam /= 0.8\n",
    "\n",
    "        cam = cv2.resize(cam, (model_image_size[1],model_image_size[0]))\n",
    "        heatmap = cv2.applyColorMap(np.uint8(255*cam), cv2.COLORMAP_JET)\n",
    "        heatmap[np.where(cam <= 0.2)] = 0\n",
    "\n",
    "        out = cv2.addWeighted(img, 0.8, heatmap, 0.4, 0)\n",
    "\n",
    "        plt.axis('off')\n",
    "        plt.imshow(out[:,:,::-1])\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total layers in the model: 178\n",
      "final_layer index: 177\n"
     ]
    }
   ],
   "source": [
    "print(\"Total layers in the model:\", len(model.layers))\n",
    "print(\"final_layer index:\", final_layer)\n",
    "if final_layer >= len(model.layers):\n",
    "    print(f\"Error: final_layer index {final_layer} is out of range. Model only has {len(model.layers)} layers.\")\n",
    "else:\n",
    "    weights = model.layers[final_layer].get_weights()[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layer_output KerasTensor(type_spec=TensorSpec(shape=(None, 8, 12, 2048), dtype=tf.float32, name=None), name='conv5_block3_3_bn/FusedBatchNormV3:0', description=\"created by layer 'conv5_block3_3_bn'\")\n",
      "weights shape (2048, 10)\n",
      "79726\n"
     ]
    }
   ],
   "source": [
    "weights = model.layers[final_layer].get_weights()[0]\n",
    "layer_output = model.layers[visual_layer].output\n",
    "model2 = Model(model.input, [layer_output, model.output])\n",
    "print(\"layer_output {0}\".format(layer_output))\n",
    "print(\"weights shape {0}\".format(weights.shape))\n",
    "show_heatmap_image(model2, weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_kaggle_csv(model,  model_image_size, csv_name):\n",
    "    dir = \"/ext/Data/distracted_driver_detection/\"\n",
    "\n",
    "    gen = ImageDataGenerator()\n",
    "    test_generator = gen.flow_from_directory(dir + \"test/\",  model_image_size, shuffle=False, \n",
    "                                             batch_size=batch_size, class_mode=None)\n",
    "#     s = test_generator.__dict__\n",
    "#     del s['filenames']\n",
    "#     print(s)\n",
    "    y_pred = model.predict_generator(test_generator,  steps=test_generator.samples//batch_size+1,  verbose=1)\n",
    "    print(\"y_pred shape {}\".format(y_pred.shape))\n",
    "    y_pred = y_pred.clip(min=0.005, max=0.995)\n",
    "    print(y_pred[:3])\n",
    "\n",
    "    l = list()\n",
    "    for i, fname in enumerate(test_generator.filenames):\n",
    "        name = fname[fname.rfind('/')+1:]\n",
    "        l.append( [name, *y_pred[i]] )\n",
    "\n",
    "    l = np.array(l)\n",
    "    data = {'img': l[:,0]}\n",
    "    for i in range(10):\n",
    "        data[\"c%d\"%i] = l[:,i+1]\n",
    "    df = pd.DataFrame(data, columns=['img'] + ['c%d'%i for i in range(10)])\n",
    "    df.head(10)\n",
    "    df = df.sort_values(by='img')\n",
    "    df.to_csv(csv_name, index=None, float_format='%.3f')\n",
    "    print(\"csv saved\")\n",
    "\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_kaggle_csv(model,  model_image_size, 'csv/resnet50-imagenet-finetune{}-pred.csv'.format(fine_tune_layer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
